{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d088e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\selena huang\\anaconda3\\lib\\site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\selena huang\\anaconda3\\lib\\site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\selena huang\\anaconda3\\lib\\site-packages (from torchvision) (3.10.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\selena huang\\anaconda3\\lib\\site-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: torch==1.11.0 in c:\\users\\selena huang\\anaconda3\\lib\\site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\selena huang\\anaconda3\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\selena huang\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\selena huang\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\selena huang\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb2fa91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "files = zipfile.ZipFile('train.zip','r')\n",
    "files.extractall(os.getcwd())\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35541cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 0, 'dog': 1}\n",
      "number of class: 2\n",
      "==>>> total training batch number: 196\n",
      "==>>> total testing batch number: 98\n",
      "========================================\n",
      "[1, 1] loss: 0.878\n",
      "[1, 40] loss: 1.355\n",
      "[1, 79] loss: 0.000\n",
      "[1, 118] loss: 29947.115\n",
      "[1, 157] loss: 0.000\n",
      "[1, 196] loss: 0.000\n",
      "[2, 1] loss: 107215.270\n",
      "[2, 40] loss: nan\n",
      "[2, 79] loss: nan\n",
      "[2, 118] loss: nan\n",
      "[2, 157] loss: nan\n",
      "[2, 196] loss: nan\n",
      "[3, 1] loss: nan\n",
      "[3, 40] loss: nan\n",
      "[3, 79] loss: nan\n",
      "[3, 118] loss: nan\n",
      "[3, 157] loss: nan\n",
      "[3, 196] loss: nan\n",
      "[4, 1] loss: nan\n",
      "[4, 40] loss: nan\n",
      "[4, 79] loss: nan\n",
      "[4, 118] loss: nan\n",
      "[4, 157] loss: nan\n",
      "[4, 196] loss: nan\n",
      "[5, 1] loss: nan\n",
      "[5, 40] loss: nan\n",
      "[5, 79] loss: nan\n",
      "[5, 118] loss: nan\n",
      "[5, 157] loss: nan\n",
      "[5, 196] loss: nan\n",
      "[6, 1] loss: nan\n",
      "[6, 40] loss: nan\n",
      "[6, 79] loss: nan\n",
      "[6, 118] loss: nan\n",
      "[6, 157] loss: nan\n",
      "[6, 196] loss: nan\n",
      "[7, 1] loss: nan\n",
      "[7, 40] loss: nan\n",
      "[7, 79] loss: nan\n",
      "[7, 118] loss: nan\n",
      "[7, 157] loss: nan\n",
      "[7, 196] loss: nan\n",
      "[8, 1] loss: nan\n",
      "[8, 40] loss: nan\n",
      "[8, 79] loss: nan\n",
      "[8, 118] loss: nan\n",
      "[8, 157] loss: nan\n",
      "[8, 196] loss: nan\n",
      "[9, 1] loss: nan\n",
      "[9, 40] loss: nan\n",
      "[9, 79] loss: nan\n",
      "[9, 118] loss: nan\n",
      "[9, 157] loss: nan\n",
      "[9, 196] loss: nan\n",
      "[10, 1] loss: nan\n",
      "[10, 40] loss: nan\n",
      "[10, 79] loss: nan\n",
      "[10, 118] loss: nan\n",
      "[10, 157] loss: nan\n",
      "[10, 196] loss: nan\n",
      "[11, 1] loss: nan\n",
      "[11, 40] loss: nan\n",
      "[11, 79] loss: nan\n",
      "[11, 118] loss: nan\n",
      "[11, 157] loss: nan\n",
      "[11, 196] loss: nan\n",
      "[12, 1] loss: nan\n",
      "[12, 40] loss: nan\n",
      "[12, 79] loss: nan\n",
      "[12, 118] loss: nan\n",
      "[12, 157] loss: nan\n",
      "[12, 196] loss: nan\n",
      "[13, 1] loss: nan\n",
      "[13, 40] loss: nan\n",
      "[13, 79] loss: nan\n",
      "[13, 118] loss: nan\n",
      "[13, 157] loss: nan\n",
      "[13, 196] loss: nan\n",
      "[14, 1] loss: nan\n",
      "[14, 40] loss: nan\n",
      "[14, 79] loss: nan\n",
      "[14, 118] loss: nan\n",
      "[14, 157] loss: nan\n",
      "[14, 196] loss: nan\n",
      "[15, 1] loss: nan\n",
      "[15, 40] loss: nan\n",
      "[15, 79] loss: nan\n",
      "[15, 118] loss: nan\n",
      "[15, 157] loss: nan\n",
      "[15, 196] loss: nan\n",
      "[16, 1] loss: nan\n",
      "[16, 40] loss: nan\n",
      "[16, 79] loss: nan\n",
      "[16, 118] loss: nan\n",
      "[16, 157] loss: nan\n",
      "[16, 196] loss: nan\n",
      "[17, 1] loss: nan\n",
      "[17, 40] loss: nan\n",
      "[17, 79] loss: nan\n",
      "[17, 118] loss: nan\n",
      "[17, 157] loss: nan\n",
      "[17, 196] loss: nan\n",
      "[18, 1] loss: nan\n",
      "[18, 40] loss: nan\n",
      "[18, 79] loss: nan\n",
      "[18, 118] loss: nan\n",
      "[18, 157] loss: nan\n",
      "[18, 196] loss: nan\n",
      "[19, 1] loss: nan\n",
      "[19, 40] loss: nan\n",
      "[19, 79] loss: nan\n",
      "[19, 118] loss: nan\n",
      "[19, 157] loss: nan\n",
      "[19, 196] loss: nan\n",
      "[20, 1] loss: nan\n",
      "[20, 40] loss: nan\n",
      "[20, 79] loss: nan\n",
      "[20, 118] loss: nan\n",
      "[20, 157] loss: nan\n",
      "[20, 196] loss: nan\n"
     ]
    }
   ],
   "source": [
    "#201808\n",
    "#Pytorch 0.4.0\n",
    "\"\"\"\n",
    "Attention: This code has Exploding Gradients problem\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "LearningRate = 0.001\n",
    "EPOCH = 20\n",
    "BATCH = 128\n",
    "SAVE_CSV = 'eval.csv'\n",
    "TEST_DIR = './test1'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class TestImageFolder(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.imgs = []\n",
    "        for filename in os.listdir('./test1'):\n",
    "            if filename.endswith('jpg'):\n",
    "                self.imgs.append('{}'.format(filename))\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root, filename))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            return img, filename\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "\n",
    "# Data\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(227),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_dataset = datasets.ImageFolder(root='./train/', transform=transform_train)\n",
    "print(train_dataset.class_to_idx)\n",
    "\n",
    "NUM_classes = len(train_dataset.classes)\n",
    "print('number of class: %d' %NUM_classes)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=BATCH,\n",
    "                                           num_workers=0,\n",
    "                                           pin_memory=True,\n",
    "                                           )\n",
    "print('==>>> total training batch number: {}'.format(len(train_loader)))\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "test_dataset = TestImageFolder(TEST_DIR, transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH, \n",
    "                                          shuffle=False, num_workers=0)\n",
    "\n",
    "print('==>>> total testing batch number: {}'.format(len(test_loader)))\n",
    "print('========================================')\n",
    "\n",
    "\n",
    "#Local Response Normalization(LRN)\n",
    "class LRN(nn.Module):\n",
    "    def __init__(self, local_size=1, alpha=1.0, beta=0.75, ACROSS_CHANNELS=False):\n",
    "        super(LRN, self).__init__()\n",
    "        self.ACROSS_CHANNELS = ACROSS_CHANNELS\n",
    "        if self.ACROSS_CHANNELS:\n",
    "            self.average=nn.AvgPool3d(kernel_size=(local_size, 1, 1),\n",
    "                    stride=1,\n",
    "                    padding=(int((local_size-1.0)/2), 0, 0))\n",
    "        else:\n",
    "            self.average=nn.AvgPool2d(kernel_size=local_size,\n",
    "                    stride=1,\n",
    "                    padding=int((local_size-1.0)/2))\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.ACROSS_CHANNELS:\n",
    "            div = x.pow(2).unsqueeze(1)\n",
    "            div = self.average(div).squeeze(1)\n",
    "            div = div.mul(self.alpha).add(1.0).pow(self.beta)\n",
    "        else:\n",
    "            div = x.pow(2)\n",
    "            div = self.average(div)\n",
    "            div = div.mul(self.alpha).add(1.0).pow(self.beta)\n",
    "        x = x.div(div)\n",
    "        return x\n",
    "\n",
    "#AlexNet network\n",
    "class myAlexNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes = NUM_classes):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, 11, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((3,3), stride = 2),\n",
    "            LRN(local_size=5, alpha=1e-4, beta=0.75, ACROSS_CHANNELS=True),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, 3, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((3, 3), stride=2),\n",
    "            LRN(local_size=5, alpha=1e-4, beta=0.75, ACROSS_CHANNELS=True)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((3,3), stride=2),\n",
    "        )\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256*6*6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.layer8 = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer5(self.layer4(self.layer3(self.layer2(self.layer1(x)))))\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.layer8(self.layer7(self.layer6(x)))\n",
    "        return x\n",
    "\n",
    "    def name(self):\n",
    "        return \"AlexNet\"\n",
    "\n",
    "alexnet = myAlexNet().to(device)\n",
    "#print(alexnet)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = optim.SGD(alexnet.parameters(), lr=LearningRate, momentum=0.9)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #training step\n",
    "    for epoch in range(EPOCH):\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = alexnet(inputs)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 39 == 0:\n",
    "                print('[%d, %d] loss: %.03f'\n",
    "                      % (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = []\n",
    "        nameid = []\n",
    "\n",
    "        for ii, data in enumerate(test_loader):\n",
    "            images, filename = data\n",
    "            images = images.to(device)\n",
    "            outputs = alexnet(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            results.extend(predicted)\n",
    "            nameid.extend(filename)                  \n",
    "\n",
    "        # write a csv file\n",
    "        f = open(SAVE_CSV, \"w\")\n",
    "        for i in range(len(nameid)):\n",
    "            f.write(\"{} {}\\n\".format(nameid[i], results[i]))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b6eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
